{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e01a775",
   "metadata": {},
   "source": [
    "##Vishnu Koushik Tekuru, koushiktekuru@gmail.com. HMLR 2025 Data Scientist Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de9e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# NLP and ML libraries \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5699684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading all NLTK data (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\english_wordnet.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package mock_corpus to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mock_corpus.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\koush\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NLTK data downloaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def download_nltk_data():\n",
    "    \"\"\"Download all NLTK data\"\"\"\n",
    "    try:\n",
    "        print(\"Downloading all NLTK data (this may take a while)...\")\n",
    "        nltk.download('all')\n",
    "        print(\"All NLTK data downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK data: {e}\")\n",
    "\n",
    "# Download data at import time\n",
    "download_nltk_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41228dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBCAnalyzer:\n",
    "    \"\"\"\n",
    "    BBC dataset analyzer\n",
    "    Features: Better NLP, multiple classifiers, clustering, proper evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path=\"bbc\"):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.documents = []\n",
    "        self.labels = []\n",
    "        self.processed_documents = []\n",
    "        \n",
    "        # Initialize NLP tools with error handling\n",
    "        try:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            print(\"✓ NLP tools initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error initializing NLP tools: {e}\")\n",
    "            # Fallback to basic stop words\n",
    "            self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "            self.lemmatizer = None\n",
    "        \n",
    "        # Enhanced keyword dictionaries with more sophisticated patterns\n",
    "        self.subcategory_keywords = {\n",
    "            'business': {\n",
    "                'stock_market': {\n",
    "                    'primary': ['stock', 'share', 'market', 'trading', 'exchange'],\n",
    "                    'secondary': ['nasdaq', 'dow', 'ftse', 'index', 'equity', 'portfolio']\n",
    "                },\n",
    "                'company_news': {\n",
    "                    'primary': ['company', 'firm', 'corporation', 'business'],\n",
    "                    'secondary': ['ceo', 'executive', 'board', 'corporate', 'management']\n",
    "                },\n",
    "                'mergers_acquisitions': {\n",
    "                    'primary': ['merger', 'acquisition', 'takeover', 'buyout'],\n",
    "                    'secondary': ['deal', 'acquire', 'merge', 'purchase', 'consolidation']\n",
    "                },\n",
    "                'financial_results': {\n",
    "                    'primary': ['profit', 'revenue', 'earnings', 'financial'],\n",
    "                    'secondary': ['quarterly', 'annual', 'results', 'income', 'turnover']\n",
    "                },\n",
    "                'economic_policy': {\n",
    "                    'primary': ['economy', 'economic', 'policy', 'government'],\n",
    "                    'secondary': ['inflation', 'interest', 'budget', 'fiscal', 'monetary']\n",
    "                }\n",
    "            },\n",
    "            'entertainment': {\n",
    "                'cinema': {\n",
    "                    'primary': ['film', 'movie', 'cinema', 'hollywood'],\n",
    "                    'secondary': ['actor', 'actress', 'director', 'oscar', 'premiere', 'box office']\n",
    "                },\n",
    "                'television': {\n",
    "                    'primary': ['television', 'tv', 'show', 'series'],\n",
    "                    'secondary': ['channel', 'broadcast', 'programme', 'episode', 'season']\n",
    "                },\n",
    "                'music': {\n",
    "                    'primary': ['music', 'song', 'album', 'singer'],\n",
    "                    'secondary': ['band', 'concert', 'tour', 'chart', 'record', 'artist']\n",
    "                },\n",
    "                'theatre': {\n",
    "                    'primary': ['theatre', 'theater', 'play', 'stage'],\n",
    "                    'secondary': ['west end', 'broadway', 'performance', 'drama', 'musical']\n",
    "                },\n",
    "                'celebrity_news': {\n",
    "                    'primary': ['celebrity', 'star', 'famous', 'personality'],\n",
    "                    'secondary': ['gossip', 'scandal', 'relationship', 'marriage', 'divorce']\n",
    "                }\n",
    "            },\n",
    "            'sport': {\n",
    "                'football': {\n",
    "                    'primary': ['football', 'soccer', 'premier league', 'fifa'],\n",
    "                    'secondary': ['goal', 'match', 'team', 'player', 'manager', 'transfer']\n",
    "                },\n",
    "                'cricket': {\n",
    "                    'primary': ['cricket', 'test', 'wicket', 'bat'],\n",
    "                    'secondary': ['bowl', 'ashes', 'county', 'icc', 'innings', 'runs']\n",
    "                },\n",
    "                'tennis': {\n",
    "                    'primary': ['tennis', 'wimbledon', 'court', 'serve'],\n",
    "                    'secondary': ['match', 'set', 'game', 'tournament', 'ranking']\n",
    "                },\n",
    "                'olympics': {\n",
    "                    'primary': ['olympic', 'olympics', 'medal', 'gold'],\n",
    "                    'secondary': ['silver', 'bronze', 'athlete', 'games', 'torch']\n",
    "                },\n",
    "                'rugby': {\n",
    "                    'primary': ['rugby', 'scrum', 'try', 'union'],\n",
    "                    'secondary': ['league', 'world cup', 'six nations', 'tackle']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Media personality identification patterns\n",
    "        self.personality_indicators = {\n",
    "            'politician': {\n",
    "                'titles': ['minister', 'mp', 'president', 'senator', 'mayor', 'councillor'],\n",
    "                'contexts': ['government', 'parliament', 'election', 'policy', 'cabinet']\n",
    "            },\n",
    "            'actor': {\n",
    "                'titles': ['actor', 'actress', 'star', 'performer'],\n",
    "                'contexts': ['film', 'movie', 'cinema', 'role', 'character', 'performance']\n",
    "            },\n",
    "            'musician': {\n",
    "                'titles': ['singer', 'musician', 'artist', 'performer'],\n",
    "                'contexts': ['music', 'song', 'album', 'concert', 'tour', 'band']\n",
    "            },\n",
    "            'athlete': {\n",
    "                'titles': ['player', 'athlete', 'captain', 'striker', 'goalkeeper'],\n",
    "                'contexts': ['sport', 'team', 'match', 'game', 'tournament', 'championship']\n",
    "            },\n",
    "            'tv_personality': {\n",
    "                'titles': ['presenter', 'host', 'anchor', 'reporter'],\n",
    "                'contexts': ['television', 'show', 'programme', 'broadcast', 'channel']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load BBC dataset with error handling and statistics\"\"\"\n",
    "        print(\"Loading BBC dataset...\")\n",
    "        \n",
    "        categories = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "        load_stats = {}\n",
    "        \n",
    "        for category in categories:\n",
    "            category_path = os.path.join(self.dataset_path, category)\n",
    "            category_count = 0\n",
    "            \n",
    "            if os.path.exists(category_path):\n",
    "                for filename in os.listdir(category_path):\n",
    "                    if filename.endswith('.txt'):\n",
    "                        file_path = os.path.join(category_path, filename)\n",
    "                        \n",
    "                        try:\n",
    "                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                                content = file.read().strip()\n",
    "                                if len(content) > 50:  # Only keep substantial articles\n",
    "                                    self.documents.append(content)\n",
    "                                    self.labels.append(category)\n",
    "                                    category_count += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Could not read {filename}: {e}\")\n",
    "                \n",
    "                load_stats[category] = category_count\n",
    "            else:\n",
    "                print(f\"Warning: Category folder '{category}' not found\")\n",
    "                load_stats[category] = 0\n",
    "        \n",
    "        print(f\"\\nDataset loaded successfully!\")\n",
    "        print(f\"Total articles: {len(self.documents)}\")\n",
    "        print(\"\\nArticles per category:\")\n",
    "        for category, count in load_stats.items():\n",
    "            print(f\"  {category}: {count} articles\")\n",
    "        \n",
    "        return len(self.documents) > 0\n",
    "    \n",
    "    def preprocess_text(self, text, remove_stopwords=True, lemmatize=True):\n",
    "        \"\"\"\n",
    "        Enhanced text preprocessing with error handling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            \n",
    "            # Remove special characters but keep sentence structure\n",
    "            text = re.sub(r'[^a-zA-Z\\s\\.]', ' ', text)\n",
    "            \n",
    "            # Remove extra whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "            \n",
    "            # Remove stopwords if requested\n",
    "            if remove_stopwords:\n",
    "                tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
    "            \n",
    "            # Lemmatize if requested and available\n",
    "            if lemmatize and self.lemmatizer:\n",
    "                tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "            \n",
    "            return ' '.join(tokens)\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing text: {e}\")\n",
    "            # Return basic cleaned text\n",
    "            return re.sub(r'[^a-zA-Z\\s]', ' ', text.lower())\n",
    "    \n",
    "    def subcategorization(self):\n",
    "        \"\"\"\n",
    "        Enhanced sub-categorization using weighted keyword matching\n",
    "        \"\"\"\n",
    "        print(\"\\n=== SUB-CATEGORIZATION ===\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for main_category in ['business', 'entertainment', 'sport']:\n",
    "            if main_category not in self.subcategory_keywords:\n",
    "                continue\n",
    "                \n",
    "            # Get articles for this category\n",
    "            category_articles = [(i, doc) for i, doc in enumerate(self.documents) \n",
    "                               if self.labels[i] == main_category]\n",
    "            \n",
    "            if not category_articles:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nAnalyzing {main_category} articles...\")\n",
    "            subcategory_scores = defaultdict(list)\n",
    "            \n",
    "            for article_idx, article in category_articles:\n",
    "                article_lower = article.lower()\n",
    "                article_scores = {}\n",
    "                \n",
    "                # Calculate weighted scores for each subcategory\n",
    "                for subcategory, keywords in self.subcategory_keywords[main_category].items():\n",
    "                    primary_score = sum(2 for keyword in keywords['primary'] \n",
    "                                      if keyword in article_lower)\n",
    "                    secondary_score = sum(1 for keyword in keywords['secondary'] \n",
    "                                        if keyword in article_lower)\n",
    "                    \n",
    "                    total_score = primary_score + secondary_score\n",
    "                    article_scores[subcategory] = total_score\n",
    "                \n",
    "                # Assign to best matching subcategory\n",
    "                if article_scores:\n",
    "                    best_subcategory = max(article_scores, key=article_scores.get)\n",
    "                    if article_scores[best_subcategory] > 0:\n",
    "                        subcategory_scores[best_subcategory].append({\n",
    "                            'article_idx': article_idx,\n",
    "                            'score': article_scores[best_subcategory],\n",
    "                            'preview': article[:200] + \"...\"\n",
    "                        })\n",
    "            \n",
    "            results[main_category] = dict(subcategory_scores)\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n{main_category.upper()} Sub-categories:\")\n",
    "            for subcategory, articles in subcategory_scores.items():\n",
    "                print(f\"  {subcategory}: {len(articles)} articles\")\n",
    "                if articles:\n",
    "                    best_article = max(articles, key=lambda x: x['score'])\n",
    "                    print(f\"    Best match (score: {best_article['score']}): {best_article['preview']}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def extract_named_entities(self):\n",
    "        \"\"\"\n",
    "        Extract named entities using NLTK with improved error handling\n",
    "        \"\"\"\n",
    "        print(\"\\n=== NAMED ENTITY EXTRACTION ===\")\n",
    "        \n",
    "        all_entities = {'persons': [], 'organizations': [], 'locations': []}\n",
    "        media_personalities = defaultdict(list)\n",
    "        \n",
    "        # Process a sample of documents to avoid overwhelming output\n",
    "        sample_size = min(100, len(self.documents))\n",
    "        sample_indices = np.random.choice(len(self.documents), sample_size, replace=False)\n",
    "        \n",
    "        print(f\"Processing {sample_size} articles for named entities...\")\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            document = self.documents[idx]\n",
    "            category = self.labels[idx]\n",
    "            \n",
    "            try:\n",
    "                # Process first few sentences for efficiency\n",
    "                sentences = sent_tokenize(document)[:3]\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    try:\n",
    "                        # Get named entities\n",
    "                        tokens = word_tokenize(sentence)\n",
    "                        pos_tags = pos_tag(tokens)\n",
    "                        chunks = ne_chunk(pos_tags)\n",
    "                        \n",
    "                        for chunk in chunks:\n",
    "                            if hasattr(chunk, 'label'):\n",
    "                                entity_name = ' '.join([token for token, pos in chunk.leaves()])\n",
    "                                \n",
    "                                if chunk.label() == 'PERSON':\n",
    "                                    all_entities['persons'].append(entity_name)\n",
    "                                    \n",
    "                                    # Try to classify this person\n",
    "                                    personality_type = self._classify_personality(sentence, entity_name)\n",
    "                                    if personality_type:\n",
    "                                        media_personalities[personality_type].append({\n",
    "                                            'name': entity_name,\n",
    "                                            'category': category,\n",
    "                                            'context': sentence,\n",
    "                                            'confidence': self._calculate_confidence(sentence, personality_type)\n",
    "                                        })\n",
    "                                \n",
    "                                elif chunk.label() == 'ORGANIZATION':\n",
    "                                    all_entities['organizations'].append(entity_name)\n",
    "                                \n",
    "                                elif chunk.label() in ['GPE', 'GSP']:  # Geographic entities\n",
    "                                    all_entities['locations'].append(entity_name)\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing sentence for NER: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing document {idx} for NER: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Clean up and count entities\n",
    "        for entity_type in all_entities:\n",
    "            all_entities[entity_type] = list(set(all_entities[entity_type]))\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nEntities found:\")\n",
    "        for entity_type, entities in all_entities.items():\n",
    "            print(f\"  {entity_type}: {len(entities)} unique entities\")\n",
    "            if entities:\n",
    "                print(f\"    Examples: {', '.join(entities[:5])}\")\n",
    "        \n",
    "        print(f\"\\nMedia personalities by type:\")\n",
    "        for personality_type, people in media_personalities.items():\n",
    "            unique_people = {}\n",
    "            for person in people:\n",
    "                if person['name'] not in unique_people or person['confidence'] > unique_people[person['name']]['confidence']:\n",
    "                    unique_people[person['name']] = person\n",
    "            \n",
    "            print(f\"  {personality_type}: {len(unique_people)} people\")\n",
    "            for name, info in list(unique_people.items())[:5]:\n",
    "                print(f\"    - {name} (confidence: {info['confidence']:.2f})\")\n",
    "        \n",
    "        return all_entities, dict(media_personalities)\n",
    "    \n",
    "    def _classify_personality(self, sentence, person_name):\n",
    "        \"\"\"Classify a person's role based on context\"\"\"\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for personality_type, indicators in self.personality_indicators.items():\n",
    "            score = 0\n",
    "            \n",
    "            # Check for titles\n",
    "            for title in indicators['titles']:\n",
    "                if title in sentence_lower:\n",
    "                    score += 3\n",
    "            \n",
    "            # Check for context words\n",
    "            for context in indicators['contexts']:\n",
    "                if context in sentence_lower:\n",
    "                    score += 1\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = personality_type\n",
    "        \n",
    "        return best_match if best_score > 0 else None\n",
    "    \n",
    "    def _calculate_confidence(self, sentence, personality_type):\n",
    "        \"\"\"Calculate confidence score for personality classification\"\"\"\n",
    "        if personality_type not in self.personality_indicators:\n",
    "            return 0.0\n",
    "        \n",
    "        sentence_lower = sentence.lower()\n",
    "        indicators = self.personality_indicators[personality_type]\n",
    "        \n",
    "        title_matches = sum(1 for title in indicators['titles'] if title in sentence_lower)\n",
    "        context_matches = sum(1 for context in indicators['contexts'] if context in sentence_lower)\n",
    "        \n",
    "        # Normalize confidence score\n",
    "        max_possible = len(indicators['titles']) + len(indicators['contexts'])\n",
    "        actual_score = (title_matches * 3) + context_matches\n",
    "        \n",
    "        return min(actual_score / max_possible, 1.0)\n",
    "    \n",
    "    def extract_april_events(self):\n",
    "        \"\"\"\n",
    "        Extract April events with improved pattern matching and context\n",
    "        \"\"\"\n",
    "        print(\"\\n=== APRIL EVENTS EXTRACTION ===\")\n",
    "        \n",
    "        april_events = []\n",
    "        \n",
    "        # More sophisticated April patterns\n",
    "        april_patterns = [\n",
    "            r'april\\s+\\d{1,2}(?:st|nd|rd|th)?',\n",
    "            r'\\d{1,2}(?:st|nd|rd|th)?\\s+april',\n",
    "            r'in\\s+april',\n",
    "            r'during\\s+april',\n",
    "            r'throughout\\s+april',\n",
    "            r'april\\s+\\d{4}',\n",
    "            r'early\\s+april',\n",
    "            r'late\\s+april',\n",
    "            r'mid-april'\n",
    "        ]\n",
    "        \n",
    "        for i, document in enumerate(self.documents):\n",
    "            try:\n",
    "                sentences = sent_tokenize(document)\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    sentence_lower = sentence.lower()\n",
    "                    \n",
    "                    # Check if sentence contains April reference\n",
    "                    april_match = None\n",
    "                    for pattern in april_patterns:\n",
    "                        match = re.search(pattern, sentence_lower)\n",
    "                        if match:\n",
    "                            april_match = match.group()\n",
    "                            break\n",
    "                    \n",
    "                    if april_match:\n",
    "                        # Get surrounding context\n",
    "                        sentence_idx = sentences.index(sentence)\n",
    "                        context_start = max(0, sentence_idx - 1)\n",
    "                        context_end = min(len(sentences), sentence_idx + 2)\n",
    "                        context = ' '.join(sentences[context_start:context_end])\n",
    "                        \n",
    "                        # Try to identify event type\n",
    "                        event_type = self._identify_event_type(sentence_lower)\n",
    "                        \n",
    "                        april_events.append({\n",
    "                            'article_index': i,\n",
    "                            'category': self.labels[i],\n",
    "                            'april_reference': april_match,\n",
    "                            'event_type': event_type,\n",
    "                            'sentence': sentence,\n",
    "                            'context': context,\n",
    "                            'summary': self._generate_event_summary(sentence)\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing document {i} for April events: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Group by category and event type\n",
    "        events_by_category = defaultdict(list)\n",
    "        events_by_type = defaultdict(list)\n",
    "        \n",
    "        for event in april_events:\n",
    "            events_by_category[event['category']].append(event)\n",
    "            if event['event_type']:\n",
    "                events_by_type[event['event_type']].append(event)\n",
    "        \n",
    "        print(f\"Found {len(april_events)} April event references\")\n",
    "        \n",
    "        print(f\"\\nBy category:\")\n",
    "        for category, events in events_by_category.items():\n",
    "            print(f\"  {category}: {len(events)} events\")\n",
    "        \n",
    "        print(f\"\\nBy event type:\")\n",
    "        for event_type, events in events_by_type.items():\n",
    "            print(f\"  {event_type}: {len(events)} events\")\n",
    "            if events:\n",
    "                print(f\"    Example: {events[0]['summary']}\")\n",
    "        \n",
    "        return april_events\n",
    "    \n",
    "    def _identify_event_type(self, sentence):\n",
    "        \"\"\"Identify the type of event mentioned\"\"\"\n",
    "        event_keywords = {\n",
    "            'meeting': ['meeting', 'conference', 'summit', 'gathering'],\n",
    "            'launch': ['launch', 'release', 'unveil', 'debut', 'premiere'],\n",
    "            'competition': ['tournament', 'championship', 'competition', 'match', 'game'],\n",
    "            'announcement': ['announce', 'reveal', 'declare', 'statement'],\n",
    "            'performance': ['concert', 'show', 'performance', 'tour', 'festival']\n",
    "        }\n",
    "        \n",
    "        for event_type, keywords in event_keywords.items():\n",
    "            if any(keyword in sentence for keyword in keywords):\n",
    "                return event_type\n",
    "        \n",
    "        return 'other'\n",
    "    \n",
    "    def _generate_event_summary(self, sentence):\n",
    "        \"\"\"Generate a concise summary of the event\"\"\"\n",
    "        # Simple extractive summary - take first 100 characters\n",
    "        summary = sentence.strip()\n",
    "        if len(summary) > 100:\n",
    "            summary = summary[:100] + \"...\"\n",
    "        return summary\n",
    "    \n",
    "    def train_multiple_classifiers(self):\n",
    "        \"\"\"\n",
    "        Train and compare multiple classifiers with error handling\n",
    "        \"\"\"\n",
    "        print(\"\\n=== TRAINING MULTIPLE CLASSIFIERS ===\")\n",
    "        \n",
    "        # Preprocess all documents\n",
    "        print(\"Preprocessing documents...\")\n",
    "        self.processed_documents = [self.preprocess_text(doc) for doc in self.documents]\n",
    "        \n",
    "        # Create feature vectors using TF-IDF\n",
    "        print(\"Creating feature vectors...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=3000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        \n",
    "        X = vectorizer.fit_transform(self.processed_documents)\n",
    "        y = self.labels\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Define classifiers to try\n",
    "        classifiers = {\n",
    "            'Naive Bayes': MultinomialNB(alpha=0.1),\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "            'SVM': SVC(kernel='linear', random_state=42),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        best_classifier = None\n",
    "        best_score = 0\n",
    "        \n",
    "        print(\"\\nTraining and evaluating classifiers...\")\n",
    "        \n",
    "        for name, clf in classifiers.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Train classifier\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "                # Cross-validation score\n",
    "                cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "                \n",
    "                # Test score\n",
    "                test_score = clf.score(X_test, y_test)\n",
    "                \n",
    "                # Predictions for detailed analysis\n",
    "                y_pred = clf.predict(X_test)\n",
    "                \n",
    "                results[name] = {\n",
    "                    'cv_mean': cv_scores.mean(),\n",
    "                    'cv_std': cv_scores.std(),\n",
    "                    'test_score': test_score,\n",
    "                    'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "                }\n",
    "                \n",
    "                print(f\"  CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "                print(f\"  Test Score: {test_score:.3f}\")\n",
    "                \n",
    "                if test_score > best_score:\n",
    "                    best_score = test_score\n",
    "                    best_classifier = clf\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error training {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save best classifier and vectorizer\n",
    "        self.best_classifier = best_classifier\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        if best_classifier:\n",
    "            print(f\"\\nBest classifier: {type(best_classifier).__name__} with score: {best_score:.3f}\")\n",
    "        else:\n",
    "            print(\"\\nNo classifier trained successfully!\")\n",
    "        \n",
    "        return results, best_classifier, vectorizer\n",
    "    \n",
    "    def cluster_documents(self, n_clusters=10):\n",
    "        \"\"\"\n",
    "        Perform document clustering to find hidden patterns\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== DOCUMENT CLUSTERING (k={n_clusters}) ===\")\n",
    "        \n",
    "        if not hasattr(self, 'vectorizer'):\n",
    "            print(\"Training vectorizer first...\")\n",
    "            self.train_multiple_classifiers()\n",
    "        \n",
    "        # Use existing vectorizer\n",
    "        X = self.vectorizer.transform(self.processed_documents)\n",
    "        \n",
    "        # Perform k-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Analyze clusters\n",
    "        cluster_analysis = defaultdict(list)\n",
    "        \n",
    "        for i, cluster_id in enumerate(clusters):\n",
    "            cluster_analysis[cluster_id].append({\n",
    "                'doc_index': i,\n",
    "                'category': self.labels[i],\n",
    "                'preview': self.documents[i][:150] + \"...\"\n",
    "            })\n",
    "        \n",
    "        print(f\"Documents clustered into {n_clusters} groups:\")\n",
    "        \n",
    "        for cluster_id, docs in cluster_analysis.items():\n",
    "            categories = [doc['category'] for doc in docs]\n",
    "            category_dist = Counter(categories)\n",
    "            \n",
    "            print(f\"\\nCluster {cluster_id}: {len(docs)} documents\")\n",
    "            print(f\"  Category distribution: {dict(category_dist)}\")\n",
    "            \n",
    "            if docs:\n",
    "                print(f\"  Example: {docs[0]['preview']}\")\n",
    "        \n",
    "        return cluster_analysis\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"\n",
    "        Run the complete analysis with error handling\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"BBC DATASET ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load dataset\n",
    "            if not self.load_dataset():\n",
    "                print(\"Failed to load dataset. Please check the path and structure.\")\n",
    "                return None\n",
    "            \n",
    "            # Step 2: Sub-categorization\n",
    "            subcategories = self.subcategorization()\n",
    "            \n",
    "            # Step 3: Named entity extraction\n",
    "            entities, personalities = self.extract_named_entities()\n",
    "            \n",
    "            # Step 4: April events extraction\n",
    "            april_events = self.extract_april_events()\n",
    "            \n",
    "            # Step 5: Train multiple classifiers\n",
    "            classifier_results, best_classifier, vectorizer = self.train_multiple_classifiers()\n",
    "            \n",
    "            # Step 6: Document clustering\n",
    "            clusters = self.cluster_documents()\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ANALYSIS COMPLETE!\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Compile results\n",
    "            results = {\n",
    "                'dataset_stats': dict(Counter(self.labels)),\n",
    "                'subcategories': subcategories,\n",
    "                'entities': entities,\n",
    "                'media_personalities': personalities,\n",
    "                'april_events': april_events,\n",
    "                'classifier_results': classifier_results,\n",
    "                'best_classifier': best_classifier,\n",
    "                'vectorizer': vectorizer,\n",
    "                'document_clusters': clusters\n",
    "            }\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during analysis: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def save_results(self, results, filename='bbc_analysis_results.pkl'):\n",
    "        \"\"\"Save results to file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "            print(f\"Results saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n",
    "    \n",
    "    def classify_new_text(self, text):\n",
    "        \"\"\"Classify new text using the trained model\"\"\"\n",
    "        if not hasattr(self, 'best_classifier') or not hasattr(self, 'vectorizer'):\n",
    "            print(\"Please train the classifier first!\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            processed_text = self.preprocess_text(text)\n",
    "            text_vector = self.vectorizer.transform([processed_text])\n",
    "            \n",
    "            prediction = self.best_classifier.predict(text_vector)[0]\n",
    "            probabilities = self.best_classifier.predict_proba(text_vector)[0]\n",
    "            \n",
    "            result = {\n",
    "                'prediction': prediction,\n",
    "                'probabilities': dict(zip(self.best_classifier.classes_, probabilities))\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error classifying text: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d637b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NLP tools initialized successfully\n",
      "============================================================\n",
      "BBC DATASET ANALYSIS\n",
      "============================================================\n",
      "Loading BBC dataset...\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Total articles: 2225\n",
      "\n",
      "Articles per category:\n",
      "  business: 510 articles\n",
      "  entertainment: 386 articles\n",
      "  politics: 417 articles\n",
      "  sport: 511 articles\n",
      "  tech: 401 articles\n",
      "\n",
      "=== SUB-CATEGORIZATION ===\n",
      "\n",
      "Analyzing business articles...\n",
      "\n",
      "BUSINESS Sub-categories:\n",
      "  financial_results: 36 articles\n",
      "    Best match (score: 10): Ad sales boost Time Warner profit\n",
      "\n",
      "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
      "\n",
      "The firm, which is now one o...\n",
      "  economic_policy: 137 articles\n",
      "    Best match (score: 11): The 'ticking budget' facing the US\n",
      "\n",
      "The budget proposals laid out by the administration of US President George W Bush are highly controversial. The Washington-based Economic Policy Institute, which te...\n",
      "  company_news: 153 articles\n",
      "    Best match (score: 9): Standard Life concern at LSE bid\n",
      "\n",
      "Standard Life is the latest shareholder in Deutsche Boerse to express concern at the German stock market operator's plans to buy the London Stock Exchange.\n",
      "\n",
      "It said D...\n",
      "  mergers_acquisitions: 16 articles\n",
      "    Best match (score: 9): Qwest may spark MCI bidding war\n",
      "\n",
      "US phone company Qwest has said it will table a new offer for MCI after losing out to larger rival Verizon, setting the scene for a possible bidding war.\n",
      "\n",
      "MCI accepted...\n",
      "  stock_market: 167 articles\n",
      "    Best match (score: 12): Hariri killing hits Beirut shares\n",
      "\n",
      "Shares in Solidere, the Lebanese company founded by assassinated former prime minister Rafik Hariri, fell 15% in renewed trading in Beirut.\n",
      "\n",
      "The real estate firm, wh...\n",
      "\n",
      "Analyzing entertainment articles...\n",
      "\n",
      "ENTERTAINMENT Sub-categories:\n",
      "  theatre: 23 articles\n",
      "    Best match (score: 10): The Producers scoops stage awards\n",
      "\n",
      "The Producers has beaten Mary Poppins in the battle of the blockbuster West End musicals at the Olivier Awards.\n",
      "\n",
      "The Producers won three prizes at the UK's most pres...\n",
      "  music: 133 articles\n",
      "    Best match (score: 13): Ten-year tragedy of missing Manic\n",
      "\n",
      "Richey Edwards, guitarist and lyricist for The Manic Street Preachers, vanished 10 years ago, on 1 February 1995. His disappearance remains one of the most tragic my...\n",
      "  television: 80 articles\n",
      "    Best match (score: 11): Campaigners attack MTV 'sleaze'\n",
      "\n",
      "MTV has been criticised for \"incessant sleaze\" by television indecency campaigners in the US.\n",
      "\n",
      "The Parents Television Council (PTC), which monitors violence and sex on...\n",
      "  cinema: 144 articles\n",
      "    Best match (score: 12): Aviator wins top Globes accolades\n",
      "\n",
      "The Aviator has been named best film at the Golden Globe Awards, with its star Leonardo DiCaprio named best actor.\n",
      "\n",
      "Hollywood veteran Clint Eastwood took the best di...\n",
      "  celebrity_news: 4 articles\n",
      "    Best match (score: 5): Britney attacks 'false tabloids'\n",
      "\n",
      "Pop star Britney Spears has attacked \"false\" and \"desperate\" US tabloid magazines, questioning their honesty after they reported she was pregnant.\n",
      "\n",
      "In a letter on her...\n",
      "\n",
      "Analyzing sport articles...\n",
      "\n",
      "SPORT Sub-categories:\n",
      "  olympics: 78 articles\n",
      "    Best match (score: 12): Britain boosted by Holmes double\n",
      "\n",
      "Athletics fans endured a year of mixed emotions in 2004 as stunning victories went hand-in-hand with disappointing defeats and more drugs scandals.\n",
      "\n",
      "Kelly Holmes fina...\n",
      "  rugby: 94 articles\n",
      "    Best match (score: 11): All Black magic: New Zealand rugby\n",
      "\n",
      "Playing colours:\n",
      "\n",
      "All black\n",
      "\n",
      "The Haka and more!\n",
      "\n",
      "The All Blacks\n",
      "\n",
      "Charles John Munro discovered rugby at London's Christ College, and on his return to Nelson he stag...\n",
      "  tennis: 110 articles\n",
      "    Best match (score: 12): Mauresmo opens with victory in LA\n",
      "\n",
      "Amelie Mauresmo and Maria Sharapova won their opening matches at the Tour Championships in Los Angeles.\n",
      "\n",
      "France's Mauresmo routed Vera Zvonareva 6-1 6-0, while Wimbl...\n",
      "  football: 189 articles\n",
      "    Best match (score: 8): Parry firm over Gerrard\n",
      "\n",
      "Listen to the full interview on Sport on Five and the BBC Sport website from 1900 GMT.\n",
      "\n",
      "But Parry, speaking exclusively to BBC Sport, also admits Gerrard, who has been constan...\n",
      "  cricket: 35 articles\n",
      "    Best match (score: 4): White admits to Balco drugs link\n",
      "\n",
      "Banned American sprinter Kelli White says she knowingly took steroids given to her by Bay Area Lab Co-Operative (Balco) president Victor Conte.\n",
      "\n",
      "Conte faces a federal...\n",
      "\n",
      "=== NAMED ENTITY EXTRACTION ===\n",
      "Processing 100 articles for named entities...\n",
      "\n",
      "Entities found:\n",
      "  persons: 216 unique entities\n",
      "    Examples: Jimi Hendrix, Daily Sport, Larne, Bill Gates, Steve Wigley\n",
      "  organizations: 142 unique entities\n",
      "    Examples: Liverpool Echo, Wasps Wasps, MPs, SEC, Real Madrid\n",
      "  locations: 163 unique entities\n",
      "    Examples: British, Six, Dance, Brussels, Brits\n",
      "\n",
      "Media personalities by type:\n",
      "  tv_personality: 7 people\n",
      "    - Bill Gates (confidence: 0.11)\n",
      "    - Carol Smillie (confidence: 0.44)\n",
      "    - Jessica Taylor (confidence: 0.44)\n",
      "    - Chelsea (confidence: 0.11)\n",
      "    - Ericsson (confidence: 0.11)\n",
      "  athlete: 22 people\n",
      "    - Gates (confidence: 0.09)\n",
      "    - Wales (confidence: 0.09)\n",
      "    - Edinburgh (confidence: 0.09)\n",
      "    - Patricia (confidence: 0.09)\n",
      "    - Hewitt (confidence: 0.09)\n",
      "  actor: 29 people\n",
      "    - Austin (confidence: 0.30)\n",
      "    - Aitken (confidence: 0.30)\n",
      "    - Will (confidence: 0.40)\n",
      "    - Spike (confidence: 0.20)\n",
      "    - Lee (confidence: 0.30)\n",
      "  politician: 58 people\n",
      "    - Norwich Union International (confidence: 0.27)\n",
      "    - Yukos Russia (confidence: 0.27)\n",
      "    - Yukos (confidence: 0.27)\n",
      "    - Rosneft (confidence: 0.27)\n",
      "    - Vladimir (confidence: 0.27)\n",
      "  musician: 8 people\n",
      "    - Joss Stone (confidence: 0.40)\n",
      "    - Dizzee Rascal (confidence: 0.40)\n",
      "    - Lemar (confidence: 0.40)\n",
      "    - Jack Straw (confidence: 0.10)\n",
      "    - Michael (confidence: 0.10)\n",
      "\n",
      "=== APRIL EVENTS EXTRACTION ===\n",
      "Found 95 April event references\n",
      "\n",
      "By category:\n",
      "  business: 27 events\n",
      "  entertainment: 21 events\n",
      "  politics: 17 events\n",
      "  sport: 17 events\n",
      "  tech: 13 events\n",
      "\n",
      "By event type:\n",
      "  other: 77 events\n",
      "    Example: US-German carmaker DaimlerChrylser, a 30% shareholder in Mitsubishi Motors, decided in April 2004 no...\n",
      "  launch: 5 events\n",
      "    Example: Kirin, the fourth big name, is launching its own \"third-type\" drink in April.\n",
      "  meeting: 2 events\n",
      "    Example: He is expected to present his conclusions at an IMF meeting in Washington during April.\n",
      "  performance: 7 events\n",
      "    Example: Denmark is holding a three-day celebration of the life of the fairy-tale author, with a concert at P...\n",
      "  announcement: 4 events\n",
      "    Example: The EU has also announced that it will start accession talks with Croatia in April 2005.\n",
      "\n",
      "=== TRAINING MULTIPLE CLASSIFIERS ===\n",
      "Preprocessing documents...\n",
      "Creating feature vectors...\n",
      "\n",
      "Training and evaluating classifiers...\n",
      "\n",
      "Training Naive Bayes...\n",
      "  CV Score: 0.969 (+/- 0.013)\n",
      "  Test Score: 0.982\n",
      "\n",
      "Training Logistic Regression...\n",
      "  CV Score: 0.967 (+/- 0.012)\n",
      "  Test Score: 0.987\n",
      "\n",
      "Training SVM...\n",
      "  CV Score: 0.972 (+/- 0.016)\n",
      "  Test Score: 0.984\n",
      "\n",
      "Training Random Forest...\n",
      "  CV Score: 0.956 (+/- 0.019)\n",
      "  Test Score: 0.962\n",
      "\n",
      "Best classifier: LogisticRegression with score: 0.987\n",
      "\n",
      "=== DOCUMENT CLUSTERING (k=10) ===\n",
      "Documents clustered into 10 groups:\n",
      "\n",
      "Cluster 1: 406 documents\n",
      "  Category distribution: {'business': 401, 'entertainment': 2, 'politics': 1, 'tech': 2}\n",
      "  Example: Ad sales boost Time Warner profit\n",
      "\n",
      "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from...\n",
      "\n",
      "Cluster 2: 426 documents\n",
      "  Category distribution: {'business': 88, 'entertainment': 28, 'politics': 251, 'sport': 31, 'tech': 28}\n",
      "  Example: Yukos unit buyer faces loan claim\n",
      "\n",
      "The owners of embattled Russian oil giant Yukos are to ask the buyer of its former production unit to pay back a $9...\n",
      "\n",
      "Cluster 4: 314 documents\n",
      "  Category distribution: {'business': 6, 'politics': 3, 'tech': 305}\n",
      "  Example: Call centre users 'lose patience'\n",
      "\n",
      "Customers trying to get through to call centres are getting impatient and quicker to hang up, a survey suggests.\n",
      "\n",
      "O...\n",
      "\n",
      "Cluster 3: 167 documents\n",
      "  Category distribution: {'business': 1, 'entertainment': 164, 'tech': 2}\n",
      "  Example: Rank 'set to sell off film unit'\n",
      "\n",
      "Leisure group Rank could unveil plans to demerge its film services unit and sell its media business, reports claim.\n",
      "...\n",
      "\n",
      "Cluster 8: 162 documents\n",
      "  Category distribution: {'business': 2, 'politics': 160}\n",
      "  Example: Golden rule 'intact' says ex-aide\n",
      "\n",
      "Chancellor Gordon Brown will meet his golden economic rule \"with a margin to spare\", according to his former chief ...\n",
      "\n",
      "Cluster 6: 202 documents\n",
      "  Category distribution: {'business': 8, 'politics': 1, 'sport': 193}\n",
      "  Example: Irish duo could block Man Utd bid\n",
      "\n",
      "Irishmen JP McManus and John Magnier, who own a 29% stake in Manchester United, will reportedly reject any formal £...\n",
      "\n",
      "Cluster 7: 198 documents\n",
      "  Category distribution: {'business': 2, 'entertainment': 190, 'tech': 6}\n",
      "  Example: EMI shares hit by profit warning\n",
      "\n",
      "Shares in music giant EMI have sunk by more than 16% after the firm issued a profit warning following disappointing ...\n",
      "\n",
      "Cluster 0: 61 documents\n",
      "  Category distribution: {'business': 2, 'entertainment': 1, 'politics': 1, 'sport': 1, 'tech': 56}\n",
      "  Example: News Corp eyes video games market\n",
      "\n",
      "News Corp, the media company controlled by Australian billionaire Rupert Murdoch, is eyeing a move into the video g...\n",
      "\n",
      "Cluster 5: 157 documents\n",
      "  Category distribution: {'entertainment': 1, 'sport': 155, 'tech': 1}\n",
      "  Example: Holmes wins '2004 top TV moment'\n",
      "\n",
      "Sprinter Kelly Holmes' Olympic victory has been named the top television moment of 2004 in a BBC poll.\n",
      "\n",
      "Holmes' 800m...\n",
      "\n",
      "Cluster 9: 132 documents\n",
      "  Category distribution: {'sport': 131, 'tech': 1}\n",
      "  Example: England's defensive crisis grows\n",
      "\n",
      "England's defensive worries have deepened following the withdrawal of Tottenham's Ledley King from the squad to face...\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE!\n",
      "============================================================\n",
      "Results saved to bbc_analysis_results.pkl\n",
      "\n",
      "========================================\n",
      "CLASSIFIER DEMONSTRATION\n",
      "========================================\n",
      "\n",
      "Text: The company reported strong quarterly earnings with revenue up 15% year-over-year.\n",
      "Predicted Category: business\n",
      "Probabilities:\n",
      "  business: 0.720\n",
      "  entertainment: 0.090\n",
      "  sport: 0.082\n",
      "  tech: 0.062\n",
      "  politics: 0.045\n",
      "\n",
      "Text: The new Marvel movie broke box office records in its opening weekend.\n",
      "Predicted Category: entertainment\n",
      "Probabilities:\n",
      "  entertainment: 0.494\n",
      "  sport: 0.151\n",
      "  tech: 0.135\n",
      "  business: 0.127\n",
      "  politics: 0.094\n",
      "\n",
      "Text: Manchester United signed a new striker for £50 million in the summer transfer window.\n",
      "Predicted Category: sport\n",
      "Probabilities:\n",
      "  sport: 0.431\n",
      "  business: 0.202\n",
      "  entertainment: 0.139\n",
      "  tech: 0.135\n",
      "  politics: 0.093\n",
      "\n",
      "Text: The Prime Minister announced new environmental policies in Parliament today.\n",
      "Predicted Category: politics\n",
      "Probabilities:\n",
      "  politics: 0.550\n",
      "  business: 0.208\n",
      "  entertainment: 0.084\n",
      "  tech: 0.081\n",
      "  sport: 0.077\n",
      "\n",
      "Text: Apple unveiled its latest iPhone with improved camera technology and longer battery life.\n",
      "Predicted Category: tech\n",
      "Probabilities:\n",
      "  tech: 0.536\n",
      "  business: 0.149\n",
      "  entertainment: 0.109\n",
      "  sport: 0.107\n",
      "  politics: 0.098\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = BBCAnalyzer(r\"C:\\Users\\koush\\Desktop\\BBC\\bbc\") \n",
    "        \n",
    "        # Run complete analysis\n",
    "        results = analyzer.run_complete_analysis()\n",
    "        \n",
    "        if results:\n",
    "            # Save results\n",
    "            analyzer.save_results(results)\n",
    "            \n",
    "            # Demonstrate classifier\n",
    "            print(\"\\n\" + \"=\" * 40)\n",
    "            print(\"CLASSIFIER DEMONSTRATION\")\n",
    "            print(\"=\" * 40)\n",
    "            \n",
    "            test_texts = [\n",
    "                \"The company reported strong quarterly earnings with revenue up 15% year-over-year.\",\n",
    "                \"The new Marvel movie broke box office records in its opening weekend.\",\n",
    "                \"Manchester United signed a new striker for £50 million in the summer transfer window.\",\n",
    "                \"The Prime Minister announced new environmental policies in Parliament today.\",\n",
    "                \"Apple unveiled its latest iPhone with improved camera technology and longer battery life.\"\n",
    "            ]\n",
    "            \n",
    "            for text in test_texts:\n",
    "                result = analyzer.classify_new_text(text)\n",
    "                if result:\n",
    "                    print(f\"\\nText: {text}\")\n",
    "                    print(f\"Predicted Category: {result['prediction']}\")\n",
    "                    print(\"Probabilities:\")\n",
    "                    for category, prob in sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True):\n",
    "                        print(f\"  {category}: {prob:.3f}\")\n",
    "            \n",
    "            return results\n",
    "        else:\n",
    "            print(\"Analysis failed. Please check your dataset.\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351d46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
